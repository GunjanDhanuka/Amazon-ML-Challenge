{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-21T06:34:17.486712Z","iopub.status.busy":"2023-04-21T06:34:17.485754Z","iopub.status.idle":"2023-04-21T06:34:17.518785Z","shell.execute_reply":"2023-04-21T06:34:17.517529Z","shell.execute_reply.started":"2023-04-21T06:34:17.486637Z"},"trusted":true},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","import os, gc, re, warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T06:34:18.263789Z","iopub.status.busy":"2023-04-21T06:34:18.262700Z","iopub.status.idle":"2023-04-21T06:35:11.268357Z","shell.execute_reply":"2023-04-21T06:35:11.267128Z","shell.execute_reply.started":"2023-04-21T06:34:18.263733Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train shape: (2249698, 7) Test shape: (734736, 6) Test columns: Index(['PRODUCT_ID', 'TITLE', 'BULLET_POINTS', 'DESCRIPTION',\n","       'PRODUCT_TYPE_ID', 'src'],\n","      dtype='object')\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PRODUCT_ID</th>\n","      <th>TITLE</th>\n","      <th>BULLET_POINTS</th>\n","      <th>DESCRIPTION</th>\n","      <th>PRODUCT_TYPE_ID</th>\n","      <th>PRODUCT_LENGTH</th>\n","      <th>src</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1925202</td>\n","      <td>ArtzFolio Tulip Flowers Blackout Curtain for D...</td>\n","      <td>[LUXURIOUS &amp; APPEALING: Beautiful custom-made ...</td>\n","      <td>NaN</td>\n","      <td>1650</td>\n","      <td>2125.980000</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2673191</td>\n","      <td>Marks &amp; Spencer Girls' Pyjama Sets T86_2561C_N...</td>\n","      <td>[Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...</td>\n","      <td>NaN</td>\n","      <td>2755</td>\n","      <td>393.700000</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2765088</td>\n","      <td>PRIKNIK Horn Red Electric Air Horn Compressor ...</td>\n","      <td>[Loud Dual Tone Trumpet Horn, Compatible With ...</td>\n","      <td>Specifications: Color: Red, Material: Aluminiu...</td>\n","      <td>7537</td>\n","      <td>748.031495</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1594019</td>\n","      <td>ALISHAH Women's Cotton Ankle Length Leggings C...</td>\n","      <td>[Made By 95%cotton and 5% Lycra which gives yo...</td>\n","      <td>AISHAH Women's Lycra Cotton Ankel Leggings. Br...</td>\n","      <td>2996</td>\n","      <td>787.401574</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>283658</td>\n","      <td>The United Empire Loyalists: A Chronicle of th...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>6112</td>\n","      <td>598.424000</td>\n","      <td>train</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PRODUCT_ID                                              TITLE  \\\n","0     1925202  ArtzFolio Tulip Flowers Blackout Curtain for D...   \n","1     2673191  Marks & Spencer Girls' Pyjama Sets T86_2561C_N...   \n","2     2765088  PRIKNIK Horn Red Electric Air Horn Compressor ...   \n","3     1594019  ALISHAH Women's Cotton Ankle Length Leggings C...   \n","4      283658  The United Empire Loyalists: A Chronicle of th...   \n","\n","                                       BULLET_POINTS  \\\n","0  [LUXURIOUS & APPEALING: Beautiful custom-made ...   \n","1  [Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...   \n","2  [Loud Dual Tone Trumpet Horn, Compatible With ...   \n","3  [Made By 95%cotton and 5% Lycra which gives yo...   \n","4                                                NaN   \n","\n","                                         DESCRIPTION  PRODUCT_TYPE_ID  \\\n","0                                                NaN             1650   \n","1                                                NaN             2755   \n","2  Specifications: Color: Red, Material: Aluminiu...             7537   \n","3  AISHAH Women's Lycra Cotton Ankel Leggings. Br...             2996   \n","4                                                NaN             6112   \n","\n","   PRODUCT_LENGTH    src  \n","0     2125.980000  train  \n","1      393.700000  train  \n","2      748.031495  train  \n","3      787.401574  train  \n","4      598.424000  train  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["dftr = pd.read_csv(\"/home/gunjan/Desktop/amazon/datasetb2d9982/dataset/train.csv\")\n","dftr[\"src\"]=\"train\"\n","dfte = pd.read_csv(\"/home/gunjan/Desktop/amazon/datasetb2d9982/dataset/test.csv\")\n","dfte[\"src\"]=\"test\"\n","print('Train shape:',dftr.shape,'Test shape:',dfte.shape,'Test columns:',dfte.columns)\n","# df = pd.concat([dftr,dfte],ignore_index=True)\n","\n","dftr.head()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T06:35:11.271749Z","iopub.status.busy":"2023-04-21T06:35:11.271062Z","iopub.status.idle":"2023-04-21T06:35:12.395540Z","shell.execute_reply":"2023-04-21T06:35:12.394155Z","shell.execute_reply.started":"2023-04-21T06:35:11.271577Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(2249698, 7)\n","(2249686, 7)\n"]}],"source":["print(dftr.shape)\n","dftr = dftr.dropna(axis=0, subset=['TITLE'])\n","print(dftr.shape)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T06:35:12.398124Z","iopub.status.busy":"2023-04-21T06:35:12.397656Z","iopub.status.idle":"2023-04-21T06:35:13.334515Z","shell.execute_reply":"2023-04-21T06:35:13.333473Z","shell.execute_reply.started":"2023-04-21T06:35:12.398078Z"},"trusted":true},"outputs":[],"source":["dftr.fillna(\"\", inplace=True)\n","dfte.fillna(\"\", inplace=True)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T06:35:13.336740Z","iopub.status.busy":"2023-04-21T06:35:13.336312Z","iopub.status.idle":"2023-04-21T06:35:14.051022Z","shell.execute_reply":"2023-04-21T06:35:14.049977Z","shell.execute_reply.started":"2023-04-21T06:35:13.336697Z"},"trusted":true},"outputs":[{"data":{"text/plain":["PRODUCT_ID         0\n","TITLE              0\n","BULLET_POINTS      0\n","DESCRIPTION        0\n","PRODUCT_TYPE_ID    0\n","PRODUCT_LENGTH     0\n","src                0\n","dtype: int64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["dftr.isna().sum()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T06:35:14.054219Z","iopub.status.busy":"2023-04-21T06:35:14.053822Z","iopub.status.idle":"2023-04-21T06:35:14.240864Z","shell.execute_reply":"2023-04-21T06:35:14.239745Z","shell.execute_reply.started":"2023-04-21T06:35:14.054181Z"},"trusted":true},"outputs":[{"data":{"text/plain":["PRODUCT_ID         0\n","TITLE              0\n","BULLET_POINTS      0\n","DESCRIPTION        0\n","PRODUCT_TYPE_ID    0\n","src                0\n","dtype: int64"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["dfte.isna().sum()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T05:11:05.719635Z","iopub.status.busy":"2023-04-21T05:11:05.719009Z","iopub.status.idle":"2023-04-21T05:11:05.728676Z","shell.execute_reply":"2023-04-21T05:11:05.727446Z","shell.execute_reply.started":"2023-04-21T05:11:05.719595Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(734736, 6)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["dfte.shape"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T05:11:05.736018Z","iopub.status.busy":"2023-04-21T05:11:05.730279Z","iopub.status.idle":"2023-04-21T05:11:05.741168Z","shell.execute_reply":"2023-04-21T05:11:05.739867Z","shell.execute_reply.started":"2023-04-21T05:11:05.735978Z"},"trusted":true},"outputs":[],"source":["target_cols = ['PRODUCT_LENGTH']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-20T21:18:51.124713Z","iopub.status.busy":"2023-04-20T21:18:51.124352Z","iopub.status.idle":"2023-04-20T21:18:51.132083Z","shell.execute_reply":"2023-04-20T21:18:51.130972Z","shell.execute_reply.started":"2023-04-20T21:18:51.124676Z"},"trusted":true},"outputs":[],"source":["# import sys\n","# sys.path.append('../input/iterativestratification')\n","# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","# FOLDS = 25\n","# skf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n","# for i,(train_index, val_index) in enumerate(skf.split(dftr,dftr[target_cols])):\n","#     dftr.loc[val_index,'FOLD'] = i\n","# print('Train samples per fold:')\n","# dftr.FOLD.value_counts()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["PRODUCT_ID                                                   1925202\n","TITLE              ArtzFolio Tulip Flowers Blackout Curtain for D...\n","BULLET_POINTS      [LUXURIOUS & APPEALING: Beautiful custom-made ...\n","DESCRIPTION                                                      NaN\n","PRODUCT_TYPE_ID                                                 1650\n","PRODUCT_LENGTH                                               2125.98\n","src                                                            train\n","Name: 0, dtype: object"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["dftr.iloc[0]"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T06:37:41.806120Z","iopub.status.busy":"2023-04-21T06:37:41.805597Z","iopub.status.idle":"2023-04-21T06:37:42.396922Z","shell.execute_reply":"2023-04-21T06:37:42.395730Z","shell.execute_reply.started":"2023-04-21T06:37:41.806075Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import StratifiedKFold\n","def create_folds(data, num_splits):\n","    data[\"kfold\"] = -1\n","    data = data.sample(frac=1).reset_index(drop=True)\n","    y=data[\"PRODUCT_TYPE_ID\"]\n","    kf = StratifiedKFold(n_splits=num_splits)\n","    for f, (t_, v_) in enumerate(kf.split(X=data, y=y)):\n","        data.loc[v_, 'kfold'] = f\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train=create_folds(dftr, 5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dftr=train.loc[train.kfold.isin([0])]\n","dftr=dftr.reset_index(drop=True)\n","display(dftr.head())\n","dftr.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["del train\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# Generate Embeddings"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T05:11:10.746384Z","iopub.status.busy":"2023-04-21T05:11:10.746000Z","iopub.status.idle":"2023-04-21T05:11:22.492112Z","shell.execute_reply":"2023-04-21T05:11:22.491013Z","shell.execute_reply.started":"2023-04-21T05:11:10.746349Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModel,AutoTokenizer, DataCollatorWithPadding\n","import torch\n","import torch.nn.functional as F\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T05:11:31.423778Z","iopub.status.busy":"2023-04-21T05:11:31.422405Z","iopub.status.idle":"2023-04-21T05:11:31.429906Z","shell.execute_reply":"2023-04-21T05:11:31.428781Z","shell.execute_reply.started":"2023-04-21T05:11:31.423737Z"},"trusted":true},"outputs":[],"source":["def mean_pooling(model_output, attention_mask):\n","    token_embeddings = model_output.last_hidden_state.detach().cpu()\n","    input_mask_expanded = (\n","        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","    )\n","    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n","        input_mask_expanded.sum(1), min=1e-9\n","    )"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T05:11:42.017131Z","iopub.status.busy":"2023-04-21T05:11:42.016166Z","iopub.status.idle":"2023-04-21T05:11:42.026827Z","shell.execute_reply":"2023-04-21T05:11:42.025593Z","shell.execute_reply.started":"2023-04-21T05:11:42.017080Z"},"trusted":true},"outputs":[],"source":["BATCH_SIZE = 32\n","tokenizer = None\n","MAX_LEN = 32\n","\n","class EmbedDataset(torch.utils.data.Dataset):\n","    def __init__(self,df):\n","        self.df = df.reset_index(drop=True)\n","    def __len__(self):\n","        return len(self.df)\n","    def __getitem__(self,idx):\n","        text = self.df.loc[idx,\"TITLE\"]\n","        tokens = tokenizer(\n","                text,\n","                None,\n","                add_special_tokens=True,\n","                padding=False,\n","                truncation=True,\n","                max_length=32,return_tensors=\"pt\")\n","        tokens = {k:v.squeeze(0) for k,v in tokens.items()}\n","        return tokens\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T05:11:44.340807Z","iopub.status.busy":"2023-04-21T05:11:44.340097Z","iopub.status.idle":"2023-04-21T05:11:44.353977Z","shell.execute_reply":"2023-04-21T05:11:44.352783Z","shell.execute_reply.started":"2023-04-21T05:11:44.340768Z"},"trusted":true},"outputs":[],"source":["def get_embeddings(MODEL_NM='', MAX=640, BATCH_SIZE=4, verbose=True):\n","    global tokenizer, MAX_LEN\n","    DEVICE=\"cuda\"\n","    model = AutoModel.from_pretrained( MODEL_NM )\n","    tokenizer = AutoTokenizer.from_pretrained( MODEL_NM )\n","    MAX_LEN = MAX\n","    \n","    ds_tr = EmbedDataset(dftr)\n","    embed_dataloader_tr = torch.utils.data.DataLoader(ds_tr,\\\n","                        batch_size=BATCH_SIZE,\\\n","                        shuffle=False, collate_fn=DataCollatorWithPadding(tokenizer))\n","    ds_te = EmbedDataset(dfte)\n","    embed_dataloader_te = torch.utils.data.DataLoader(ds_te,\\\n","                        batch_size=BATCH_SIZE,\\\n","                        shuffle=False, collate_fn=DataCollatorWithPadding(tokenizer))\n","    \n","    model = model.to(DEVICE)\n","    model.eval()\n","    all_train_text_feats = []\n","    for batch in tqdm(embed_dataloader_tr,total=len(embed_dataloader_tr)):\n","        input_ids = batch[\"input_ids\"].to(DEVICE)\n","        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n","        with torch.no_grad():\n","            model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n","        sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n","        # Normalize the embeddings\n","        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n","        sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n","        all_train_text_feats.extend(sentence_embeddings)\n","    all_train_text_feats = np.array(all_train_text_feats)\n","    if verbose:\n","        print('Train embeddings shape',all_train_text_feats.shape)\n","        \n","    te_text_feats = []\n","    for batch in tqdm(embed_dataloader_te,total=len(embed_dataloader_te)):\n","        input_ids = batch[\"input_ids\"].to(DEVICE)\n","        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n","        with torch.no_grad():\n","            model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n","        sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n","        # Normalize the embeddings\n","        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n","        sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n","        te_text_feats.extend(sentence_embeddings)\n","    te_text_feats = np.array(te_text_feats)\n","    if verbose:\n","        print('Test embeddings shape',te_text_feats.shape)\n","        \n","    return all_train_text_feats, te_text_feats"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T05:11:48.416135Z","iopub.status.busy":"2023-04-21T05:11:48.415114Z","iopub.status.idle":"2023-04-21T06:23:08.930480Z","shell.execute_reply":"2023-04-21T06:23:07.171685Z","shell.execute_reply.started":"2023-04-21T05:11:48.416096Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at ../input/huggingface-deberta-variants/deberta-base/deberta-base were not used when initializing DebertaModel: ['config', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","  0%|          | 0/70303 [00:00<?, ?it/s]You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"," 29%|██▉       | 20426/70303 [17:41<42:36, 19.51it/s] IOPub message rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"," 61%|██████    | 42852/70303 [1:10:39<45:15, 10.11it/s]    \n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/2676374253.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mMODEL_NM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../input/huggingface-deberta-variants/deberta-base/deberta-base'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mall_train_text_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_text_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_24/66367852.py\u001b[0m in \u001b[0;36mget_embeddings\u001b[0;34m(MODEL_NM, MAX, BATCH_SIZE, verbose)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0msentence_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Normalize the embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    986\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m         )\n\u001b[1;32m    990\u001b[0m         \u001b[0mencoded_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    481\u001b[0m                     \u001b[0mrelative_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelative_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m                 )\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0mrelative_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelative_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         )\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mrelative_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelative_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         )\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelative_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0mrel_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m             \u001b[0mrel_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisentangled_att_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrel_att\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mdisentangled_att_bias\u001b[0;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mp2c_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_query_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             p2c_att = torch.gather(\n\u001b[0;32m--> 732\u001b[0;31m                 \u001b[0mp2c_att\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp2c_dynamic_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp2c_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             ).transpose(-1, -2)\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["MODEL_NM = '../input/huggingface-deberta-variants/deberta-base/deberta-base'\n","all_train_text_feats, te_text_feats = get_embeddings(MODEL_NM, MAX=32, BATCH_SIZE=32)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.save('train_deberta_base_feats.npy', all_train_text_feats)\n","np.save('test_deberta_base_feats.npy', te_text_feats)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["del all_train_text_feats, te_text_feats\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MODEL_NM = '../input/deberta-v3-large/deberta-v3-large'\n","all_train_text_feats2, te_text_feats2 = get_embeddings(MODEL_NM, MAX=32, BATCH_SIZE=32)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.save('train_deberta_largev3_feats.npy', all_train_text_feats2)\n","np.save('test_deberta_largev3_feats.npy', te_text_feats2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["del all_train_text_feats2, te_text_feats2\n","gc.collect()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":4}
