{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T10:20:28.695080Z","iopub.status.busy":"2023-04-21T10:20:28.694401Z","iopub.status.idle":"2023-04-21T10:20:43.405939Z","shell.execute_reply":"2023-04-21T10:20:43.403044Z","shell.execute_reply.started":"2023-04-21T10:20:28.695034Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import pandas as pd\n","import numpy as np\n","import csv\n","import math\n","import os\n","import random\n","import time\n","import copy\n","from tqdm.notebook import tqdm\n","import multiprocessing\n","import yaml\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchmetrics import MeanAbsolutePercentageError\n","\n","from sklearn.model_selection import train_test_split\n","\n","from transformers import AutoModel, AutoTokenizer, AutoConfig\n","from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n","# from transformers import Trainer, TrainingArguments\n","from transformers import DataCollatorWithPadding"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T10:20:43.417338Z","iopub.status.busy":"2023-04-21T10:20:43.416276Z","iopub.status.idle":"2023-04-21T10:20:43.427334Z","shell.execute_reply":"2023-04-21T10:20:43.426137Z","shell.execute_reply.started":"2023-04-21T10:20:43.417284Z"},"trusted":true},"outputs":[],"source":["os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T10:20:43.461886Z","iopub.status.busy":"2023-04-21T10:20:43.461148Z","iopub.status.idle":"2023-04-21T10:20:43.547748Z","shell.execute_reply":"2023-04-21T10:20:43.546467Z","shell.execute_reply.started":"2023-04-21T10:20:43.461837Z"},"trusted":true},"outputs":[],"source":["class Config:\n","    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","    model_name = \"xlm-roberta-base\"\n","    model_save_name = \"best_model.pth\"\n","    train_batch_size = 16\n","    valid_batch_size = 16\n","    grad_max_norm = 1000\n","    n_accumulate = 1\n","    epochs = 4\n","    collate_fn = None\n","    weight_decay = 1e-5\n","    lr = 1e-4\n","    min_lr = 6e-5\n","    seed = 42\n","    max_len = 32\n","    num_workers = 0\n","    pooling_method = 'weighted'\n","    folds = '01'\n","    debug = False\n","    wandb = True\n","    log_steps = 1000\n","\n","config = Config()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def class2dict(f):\n","    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n","\n","if config.wandb:\n","    import wandb\n","\n","    try:\n","        from kaggle_secrets import UserSecretsClient\n","        user_secrets = UserSecretsClient()\n","        secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n","        wandb.login(key=secret_value_0)\n","        anony = None\n","    except:\n","        anony = \"must\"\n","        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n","\n","    run = wandb.init(project='amazon_ml',\n","                    config=class2dict(config),\n","                    job_type=\"train\",\n","                    anonymous=anony)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T10:20:43.552198Z","iopub.status.busy":"2023-04-21T10:20:43.551583Z","iopub.status.idle":"2023-04-21T10:20:45.777293Z","shell.execute_reply":"2023-04-21T10:20:45.776171Z","shell.execute_reply.started":"2023-04-21T10:20:43.552155Z"},"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n","if config.model_name == 'gpt2':\n","    tokenizer.pad_token = tokenizer.eos_token\n","config.collate_fn = DataCollatorWithPadding(tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T10:20:45.905109Z","iopub.status.busy":"2023-04-21T10:20:45.904778Z","iopub.status.idle":"2023-04-21T10:20:45.914311Z","shell.execute_reply":"2023-04-21T10:20:45.913283Z","shell.execute_reply.started":"2023-04-21T10:20:45.905079Z"},"trusted":true},"outputs":[],"source":["# set seed\n","def set_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed(config.seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T10:20:46.833104Z","iopub.status.busy":"2023-04-21T10:20:46.832005Z","iopub.status.idle":"2023-04-21T10:21:27.106230Z","shell.execute_reply":"2023-04-21T10:21:27.105037Z","shell.execute_reply.started":"2023-04-21T10:20:46.833054Z"},"trusted":true},"outputs":[],"source":["# train_fold0 = pd.read_csv(\"archive/train_fold0.csv\")\n","# train_fold1 = pd.read_csv(\"archive/train_fold1.csv\")\n","# train = pd.concat([train_fold0, train_fold1], axis=0).reset_index(drop=True)\n","train = pd.read_csv(\"dataset/train.csv\")\n","train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T10:21:27.109083Z","iopub.status.busy":"2023-04-21T10:21:27.108658Z","iopub.status.idle":"2023-04-21T10:21:27.114394Z","shell.execute_reply":"2023-04-21T10:21:27.113121Z","shell.execute_reply.started":"2023-04-21T10:21:27.109041Z"},"trusted":true},"outputs":[],"source":["target_column = \"PRODUCT_LENGTH\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-21T10:21:27.712542Z","iopub.status.idle":"2023-04-21T10:21:27.713649Z","shell.execute_reply":"2023-04-21T10:21:27.713408Z","shell.execute_reply.started":"2023-04-21T10:21:27.713364Z"},"trusted":true},"outputs":[],"source":["train_subset = train[train['PRODUCT_LENGTH']<1000]\n","train_subset.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-21T10:21:27.716268Z","iopub.status.idle":"2023-04-21T10:21:27.717470Z","shell.execute_reply":"2023-04-21T10:21:27.717200Z","shell.execute_reply.started":"2023-04-21T10:21:27.717172Z"},"trusted":true},"outputs":[],"source":["train_subset['PRODUCT_LENGTH'] = train_subset['PRODUCT_LENGTH']/100"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-21T10:21:27.718829Z","iopub.status.idle":"2023-04-21T10:21:27.720082Z","shell.execute_reply":"2023-04-21T10:21:27.719815Z","shell.execute_reply.started":"2023-04-21T10:21:27.719788Z"},"trusted":true},"outputs":[],"source":["# Drop nan values in TITLE col, rows from the dataset\n","train_subset = train_subset.dropna(subset=['TITLE'])\n","train_subset.isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T10:10:09.194397Z","iopub.status.busy":"2023-04-21T10:10:09.193856Z","iopub.status.idle":"2023-04-21T10:10:09.207898Z","shell.execute_reply":"2023-04-21T10:10:09.206126Z","shell.execute_reply.started":"2023-04-21T10:10:09.194342Z"},"trusted":true},"outputs":[],"source":["class TextDataset(Dataset):\n","    def __init__(self, data, tokenizer, mode=\"train\", max_length=None):\n","        super(TextDataset, self).__init__()\n","        self.sentence = data[\"TITLE\"].values\n","        if mode != \"test\":\n","            self.label = data[target_column].values\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.mode = mode\n","\n","    def __len__(self):\n","        return len(self.sentence)\n","\n","    def __getitem__(self,idx):\n","        inp_tokens = self.tokenizer(self.sentence[idx], \n","                                              padding=False, \n","                                              add_special_tokens=True,\n","                                              max_length=self.max_length,\n","                                              truncation=True)\n","        item={\n","            \"input_ids\":torch.tensor(inp_tokens.input_ids,dtype=torch.long),\n","            \"attention_mask\":torch.tensor(inp_tokens.attention_mask,dtype=torch.long)\n","        }\n","\n","        if self.mode != \"test\":\n","            item['labels'] = torch.tensor(self.label[idx], dtype=torch.long)\n","\n","        return item"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T10:10:45.634537Z","iopub.status.busy":"2023-04-21T10:10:45.634115Z","iopub.status.idle":"2023-04-21T10:10:45.655038Z","shell.execute_reply":"2023-04-21T10:10:45.653506Z","shell.execute_reply.started":"2023-04-21T10:10:45.634499Z"},"trusted":true},"outputs":[],"source":["class MeanPooling(nn.Module):\n","    def __init__(self):\n","        super(MeanPooling, self).__init__()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        mean_embeddings = sum_embeddings / sum_mask\n","        return mean_embeddings\n","    \n","class MaxPooling(nn.Module):\n","    def __init__(self):\n","        super(MaxPooling, self).__init__()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        last_hidden_state[input_mask_expanded == 0] = -1e9\n","        max_embeddings = torch.max(last_hidden_state, 1)[0]\n","        return max_embeddings\n","\n","    \n","class ConcatPooling(nn.Module):\n","    def __init__(self):\n","        super(ConcatPooling, self).__init__()\n","        \n","    def forward(self, all_hidden_states):\n","        concatenate_pooling = torch.cat(\n","            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n","        )\n","        concat_embeddings = concatenate_pooling[:, 0]\n","        return concat_embeddings\n","\n","    \n","class WeightedLayerPooling(nn.Module):\n","    def __init__(self, num_hidden_layers, layer_start: int = 9, layer_weights = None):\n","        super(WeightedLayerPooling, self).__init__()\n","        self.layer_start = layer_start\n","        self.num_hidden_layers = num_hidden_layers\n","        self.layer_weights = layer_weights if layer_weights is not None \\\n","            else nn.Parameter(\n","                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n","            )\n","\n","    def forward(self, all_hidden_states):\n","        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n","        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n","        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n","        return weighted_average\n","\n","    \n","class LSTMPooling(nn.Module):\n","    def __init__(self, num_layers, hidden_size, hiddendim_lstm=256):\n","        super(LSTMPooling, self).__init__()\n","        self.num_hidden_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.hiddendim_lstm = hiddendim_lstm\n","        self.lstm = nn.LSTM(self.hidden_size, self.hiddendim_lstm, batch_first=True)\n","        self.dropout = nn.Dropout(0.1)\n","    \n","    def forward(self, all_hidden_states):\n","        ## forward\n","        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n","                                     for layer_i in range(1, self.num_hidden_layers+1)], dim=-1)\n","        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n","        out, _ = self.lstm(hidden_states, None)\n","        out = self.dropout(out[:, -1, :])\n","        return out\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T10:15:26.027272Z","iopub.status.busy":"2023-04-21T10:15:26.026813Z","iopub.status.idle":"2023-04-21T10:15:26.047634Z","shell.execute_reply":"2023-04-21T10:15:26.046512Z","shell.execute_reply.started":"2023-04-21T10:15:26.027230Z"},"trusted":true},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.model_config = AutoConfig.from_pretrained(config.model_name, output_hidden_states=True)\n","        self.model_config.hidden_dropout = 0.\n","        self.model_config.hidden_dropout_prob = 0.\n","        self.model_config.attention_dropout = 0.\n","        self.model_config.attention_probs_dropout_prob = 0.\n","       \n","        self.model = AutoModel.from_pretrained(config.model_name, config=self.model_config)\n","        \n","        if config.pooling_method == \"mean\":\n","            self.pool = MeanPooling()\n","            self.fc = nn.Linear(self.model_config.hidden_size, 1)\n","            \n","        elif config.pooling_method == 'max':\n","            self.pool = MaxPooling()\n","            self.fc = nn.Linear(self.model_config.hidden_size, 1)\n","\n","        elif config.pooling_method == 'weighted':\n","            self.pool = WeightedLayerPooling(num_hidden_layers=self.model_config.num_hidden_layers, layer_start=9)\n","            self.fc = nn.Linear(self.model_config.hidden_size, 1)\n","\n","        elif config.pooling_method == 'concat':\n","            self.pool = ConcatPooling()\n","            self.fc = nn.Linear(self.model_config.hidden_size*4, 1, )\n","\n","            \n","        elif config.pooling_method == 'lstm':\n","            hidden_lstm_dim = 512\n","            self.pool = LSTMPooling(self.model_config.num_hidden_layers, \n","                                    self.model_config.hidden_size, hidden_lstm_dim)\n","            self.fc = nn.Linear(hidden_lstm_dim, 1)\n","    \n","\n","        if config.model_name in ['microsoft/deberta-v3-base', 'xlm-roberta-base', 'roberta-base']:\n","            self.model.embeddings.requires_grad_(False)\n","            self.model.encoder.layer[:9].requires_grad_(False)\n","            \n","        elif config.model_name in ['roberta-large', 'microsoft/deberta-v3-large']:\n","            self.model.embeddings.requires_grad_(False)\n","            self.model.encoder.layer[:18].requires_grad_(False) \n","\n","        elif 'gpt2' == config.model_name:\n","            self.model.wte.requires_grad_(False)\n","            self.model.h[:9].requires_grad_(False)\n","        \n","        elif 'distilbert-base-uncased' == config.model_name:\n","            self.model.embeddings.requires_grad_(False)\n","            self.model.transformer.layer[:1].requires_grad_(False)\n","        \n","        \n","        self._init_weights(self.fc)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        \n","    def feature(self, inputs):\n","        outputs = self.model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n","        \n","        if self.config.pooling_method in ['mean', 'max']:\n","            last_hidden_states = outputs['last_hidden_state']\n","            pool_features = self.pool(last_hidden_states, inputs['attention_mask'])\n","        elif self.config.pooling_method in ['weighted', 'lstm', 'concat']:\n","            all_hidden_states = torch.stack(outputs['hidden_states'])\n","            pool_features = self.pool(all_hidden_states)\n","            if self.config.pooling_method == 'weighted':\n","                pool_features = pool_features[:, 0]\n","        \n","        return pool_features\n","\n","    def forward(self, inputs):\n","        feature = self.feature(inputs)\n","        output = self.fc(feature)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T10:11:42.853656Z","iopub.status.busy":"2023-04-21T10:11:42.853216Z","iopub.status.idle":"2023-04-21T10:11:42.861702Z","shell.execute_reply":"2023-04-21T10:11:42.860148Z","shell.execute_reply.started":"2023-04-21T10:11:42.853613Z"},"trusted":true},"outputs":[],"source":["class AvgMeter:\n","    def __init__(self, name=\"Metric\"):\n","        self.name = name\n","        self.reset()\n","    \n","    def reset(self):\n","        self.avg, self.sum, self.count = [0]*3\n","    \n","    def update(self, val, count=1):\n","        self.count += count\n","        self.sum += val * count\n","        self.avg = self.sum / self.count\n","    \n","    def __repr__(self):\n","        text = f\"{self.name}: {self.avg:.4f}\"\n","        return text\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T10:16:01.195857Z","iopub.status.busy":"2023-04-21T10:16:01.195310Z","iopub.status.idle":"2023-04-21T10:16:01.211631Z","shell.execute_reply":"2023-04-21T10:16:01.210150Z","shell.execute_reply.started":"2023-04-21T10:16:01.195818Z"},"trusted":true},"outputs":[],"source":["def one_epoch(model, criterion, dataloader, epoch, scaler=None, optimizer=None, scheduler=None, metric=None, mode='train'):\n","    \n","    loss_meter = AvgMeter()\n","    mape_meter = None\n","    if metric:\n","        mape_meter = AvgMeter()\n","    \n","    bar = tqdm(dataloader, total=len(dataloader))\n","    \n","    for idx, batch in enumerate(bar):\n","        batch = {k: v.to(config.device) for k, v in batch.items()}\n","        \n","        with torch.autocast(device_type='cuda', dtype=torch.float16):\n","            preds = model(batch)\n","  \n","        loss = criterion(preds, batch['labels'].unsqueeze(-1))\n","        \n","        if mode == \"train\":\n","            loss = loss/config.n_accumulate\n","            scaler.scale(loss).backward()\n","            if (idx+1)%config.n_accumulate==0 or (idx+1)==len(dataloader):\n","                scaler.unscale_(optimizer)\n","                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_max_norm)\n","                scaler.step(optimizer)\n","                scaler.update()\n","                for param in model.parameters():\n","                    param.grad = None\n","\n","            if scheduler:\n","                scheduler.step()\n","        if metric: \n","            mape = metric(preds, batch['labels'].unsqueeze(-1))\n","                \n","        count = batch['input_ids'].shape[0]\n","        loss_meter.update(loss.item(), count)\n","        if metric:\n","            mape_meter.update(mape.item(), count)\n","        \n","        \n","        if mode == \"train\":\n","            if metric:\n","                bar.set_postfix(epoch=epoch, train_loss=loss_meter.avg, mape=mape_meter.avg, lr=get_lr(optimizer))\n","            else:\n","                bar.set_postfix(epoch=epoch, train_loss=loss_meter.avg, lr=get_lr(optimizer))\n","        else:\n","            if metric:\n","                bar.set_postfix(epoch=epoch, valid_loss=loss_meter.avg, mape=mape_meter.avg)\n","            else:\n","                bar.set_postfix(epoch=epoch, valid_loss=loss_meter.avg)\n","\n","        if (idx+1)%config.log_steps==0:\n","            print(f\"epoch: {epoch}, iter: {idx+1}, loss: {loss_meter.avg:.4f}\")\n","            if config.wandb:\n","                wandb.log({\"epoch\": epoch+1,\"step_loss\": loss_meter.avg, \"step\": idx+1})\n","    \n","    return (loss_meter, mape_meter)\n","\n","def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group[\"lr\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T10:14:27.450974Z","iopub.status.busy":"2023-04-21T10:14:27.450572Z","iopub.status.idle":"2023-04-21T10:14:27.461297Z","shell.execute_reply":"2023-04-21T10:14:27.459861Z","shell.execute_reply.started":"2023-04-21T10:14:27.450939Z"},"trusted":true},"outputs":[],"source":["def train_eval(epochs, model, train_loader, valid_loader, \n","               criterion, optimizer, scheduler=None, scaler=None, metric=None):\n","    \n","    best_loss = np.inf\n","    best_model_weights = copy.deepcopy(model.state_dict())\n","    \n","    for epoch in range(epochs):\n","        print(f\"Epoch {epoch + 1}\")\n","        \n","        model.train()\n","        train_loss, train_mape = one_epoch(model, \n","                                          criterion, \n","                                          train_loader, \n","                                          epoch,\n","                                          scaler,\n","                                          optimizer=optimizer,\n","                                          scheduler=scheduler,\n","                                          metric=metric,\n","                                          mode=\"train\")                     \n","        model.eval()\n","        with torch.no_grad():\n","            valid_loss, valid_mape = one_epoch(model, \n","                                              criterion, \n","                                              valid_loader, \n","                                              epoch,\n","                                              optimizer=None,\n","                                              scheduler=None,\n","                                              metric=metric,\n","                                              mode=\"valid\")\n","        if config.wandb:\n","            wandb.log({\n","                f\"[fold_{config.folds}] epoch\": epoch+1,\n","                f\"[fold_{config.folds}] epoch_train_loss\": train_loss.avg,\n","                f\"[fold_{config.folds}] epoch_valid_loss\": valid_loss.avg,\n","            })\n","        \n","        if valid_loss.avg < best_loss:\n","            best_loss = valid_loss.avg\n","            best_model_weights = copy.deepcopy(model.state_dict())\n","            torch.save(model.state_dict(), f'{config.model_save_name}')\n","            print(\"Saved best model!\")\n","        \n","        print(\"=\" * 30)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T10:14:28.255606Z","iopub.status.busy":"2023-04-21T10:14:28.255194Z","iopub.status.idle":"2023-04-21T10:14:28.263072Z","shell.execute_reply":"2023-04-21T10:14:28.261560Z","shell.execute_reply.started":"2023-04-21T10:14:28.255572Z"},"trusted":true},"outputs":[],"source":["def optimizer_params(model, config=config):\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': config.weight_decay},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","    ]\n","    return optimizer_parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T10:14:32.944386Z","iopub.status.busy":"2023-04-21T10:14:32.943930Z","iopub.status.idle":"2023-04-21T10:14:33.833902Z","shell.execute_reply":"2023-04-21T10:14:33.832716Z","shell.execute_reply.started":"2023-04-21T10:14:32.944345Z"},"trusted":true},"outputs":[],"source":["train_df, valid_df = train_test_split(train_subset, \n","                                      test_size=0.33, \n","                                      shuffle=True, \n","                                      random_state=config.seed)\n","\n","train_df=train_df.reset_index(drop=True)\n","valid_df=valid_df.reset_index(drop=True)\n","\n","if config.debug:\n","    train_df = train_df.sample(100)\n","    valid_df = valid_df.sample(100)\n","\n","train_dataset = TextDataset(train_df, tokenizer, max_length=config.max_len)\n","train_loader = DataLoader(train_dataset, \n","                        batch_size=config.train_batch_size, \n","                        num_workers=config.num_workers, \n","                        shuffle=True,\n","                        collate_fn=config.collate_fn)\n","\n","valid_dataset = TextDataset(valid_df, tokenizer, max_length=config.max_len)\n","valid_loader = DataLoader(valid_dataset, \n","                        batch_size=config.valid_batch_size, \n","                        num_workers=config.num_workers, \n","                        shuffle=False,\n","                        collate_fn=config.collate_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gc"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T10:16:12.777958Z","iopub.status.busy":"2023-04-21T10:16:12.777491Z","iopub.status.idle":"2023-04-21T10:16:12.784790Z","shell.execute_reply":"2023-04-21T10:16:12.783018Z","shell.execute_reply.started":"2023-04-21T10:16:12.777919Z"},"trusted":true},"outputs":[],"source":["def criterion(logits, labels):\n","    loss = nn.L1Loss()(logits, labels)\n","    return loss \n","\n","# metric = MeanAbsolutePercentageError().to(config.device)\n","metric = None"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T10:16:14.645722Z","iopub.status.busy":"2023-04-21T10:16:14.645315Z","iopub.status.idle":"2023-04-21T10:16:14.653104Z","shell.execute_reply":"2023-04-21T10:16:14.651939Z","shell.execute_reply.started":"2023-04-21T10:16:14.645688Z"},"trusted":true},"outputs":[],"source":["num_steps = int(len(train_loader)*config.epochs)\n","model = CustomModel(config).to(config.device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_steps, eta_min=config.min_lr)\n","scaler = torch.cuda.amp.GradScaler()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-21T10:16:15.364623Z","iopub.status.busy":"2023-04-21T10:16:15.363968Z","iopub.status.idle":"2023-04-21T10:16:16.250286Z","shell.execute_reply":"2023-04-21T10:16:16.248137Z","shell.execute_reply.started":"2023-04-21T10:16:15.364585Z"},"trusted":true},"outputs":[],"source":["train_eval(config.epochs, model, train_loader, valid_loader, \n","               criterion, optimizer, scheduler, scaler, metric)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_df = pd.read_csv(\"dataset/test.csv\")\n","null_test = test_df[test_df['TITLE'].isna()]\n","test_without_null = test_df.drop(null_test.index, 0)\n","\n","if config.debug:\n","    test_without_null = test_without_null.sample(100)\n","\n","test_dataset = TextDataset(test_without_null, tokenizer, max_length=config.max_len, mode='test')\n","test_loader = DataLoader(test_dataset,\n","                        batch_size=16, \n","                        num_workers=config.num_workers,\n","                        shuffle=False, \n","                        collate_fn=DataCollatorWithPadding(tokenizer, padding='longest'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def prediction(dataloader, model, device=config.device):\n","    preds = []\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader):\n","            batch = {k:v.to(device) for k,v in batch.items()}\n","            with torch.autocast(device_type='cuda', dtype=torch.float16):\n","                pred = model(batch)\n","            preds.append(pred.cpu().numpy())\n","    preds = np.concatenate(preds)\n","    return preds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preds = prediction(test_loader, model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sub = pd.DataFrame()\n","sub['PRODUCT_ID'] = test_df['PRODUCT_ID']\n","sub['PRODUCT_LENGTH'] = preds*100\n","null_test['PRODUCT_LENGTH'] = 600\n","null_test_preds = null_test[['PRODUCT_ID', 'PRODUCT_LENGTH']]\n","sub_wo_clip = pd.concat([sub, null_test_preds] , axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sub_wo_clip.to_csv(\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import mean_absolute_percentage_error\n","\n","train_fold4 = pd.read_csv(\"archive/train4.csv\")\n","if config.debug:\n","    train_fold4 = train_fold4.sample(100)\n","train_fold4 = train_fold4.dropna(subset=['TITLE'])\n","train_fold4 = train_fold4.reset_index(drop=True)\n","\n","fold_val_dataset = TextDataset(train_fold4, tokenizer, max_length=config.max_len, mode='test')\n","fold_val_loader = DataLoader(fold_val_dataset,\n","                        batch_size=16, \n","                        num_workers=config.num_workers,\n","                        shuffle=False, \n","                        collate_fn=DataCollatorWithPadding(tokenizer, padding='longest'))\n","\n","preds_val = prediction(fold_val_loader, model)\n","\n","print(mean_absolute_percentage_error(train_fold4['PRODUCT_LENGTH'].values.reshape((-1,1)), preds_val*100))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":4}
